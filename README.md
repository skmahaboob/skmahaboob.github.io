# Mahaboob Sheik - Portfolio

Welcome to my personal portfolio website! This website is a showcase of my skills, projects, certifications, and resume. I built this site to provide a central place where potential employers, collaborators, and peers can learn more about me and my work.

## 🚀 About Me

I'm a passionate Data Engineer  with over 3 years of experience in transforming complex datasets into actionable insights. My expertise spans across big data technologies, cloud platforms, and advanced data processing tools, driving efficient data solutions that empower businesses to thrive.

- **LinkedIn**: [LinkedIn](https://www.linkedin.com/in/mahaboob-sheik)
- **GitHub**: [GitHub](https://github.com/skmahaboob)

## 💻 Skills

- **Programming Languages**: Python, SQL, Shell Scripting,Scala
- **Big Data Technologies**: Apache Spark, PySpark, Hadoop
- **Cloud Platforms**: Azure (Data Factory, Synapse Analytics, Databricks), GCP, Snowflake
- **Data Engineering Tools**: Apache Kafka, Airflow, Docker
- **CI/CD**: Azure DevOps, GitHub Actions
- **Other**: Data Warehousing, ETL, Data Modeling,StreamSets,Linux

## 🛠️ Projects

### 1. Mavericks - DataHydration
**Description**: This project focuses on designing and implementing scalable big data processing pipelines using Hadoop and Spark (with PySpark) on Google Cloud Platform's Dataproc clusters.
📊 The primary objective is to load structured data from Enterprise Data Warehouses (EDW) as the source and transform it for storage in Azure Data Lake Storage (ADLS).
🛠️ The project emphasizes efficient data handling and processing to ensure compatibility and optimal storage in the ADLS environment. 

**💾Key Components and Technologies:**

- 🗄️ Hadoop: Distributed storage and processing framework for handling large datasets.
- ⚡ Spark with PySpark: Fast and flexible big data processing engine utilizing Python APIs.
- ☁️ Google Cloud Platform (GCP)Dataproc : Managed Spark and Hadoop service for processing big data.
- 🏢 Enterprise Data Warehouses (EDW): Source of structured data from various enterprise systems.
- 🔒 Azure Data Lake Storage (ADLS): Scalable and secure storage solution for big data in Azure.
- 🔄 Data Transformation: Includes cleansing, integration, and aggregation to prepare data for storage in ADLS.
- 📈 Scalability and Performance: Design considerations to handle large volumes of data efficiently using Dataproc clusters.

**Project Goals:**
- 📊 Scalability: Design and implement pipelines that scale horizontally on GCP Dataproc clusters.
- ⚙️ Efficiency: Optimize data processing and transformation steps for performance using PySpark.
- 🔧 Reliability:Ensure robustness and fault tolerance in data loading and transformation processes.
- 🔄 Compatibility:Transform EDW data into formats suitable for storage in ADLS while maintaining data integrity.

**Deliverables:**
- 🚀 Scalable Data Processing Pipelines: Implemented using Hadoop and Spark with PySpark on Dataproc.
- 🔍 Transformation Logic:Defined and implemented using PySpark to cleanse, integrate, and transform data.
- 📄 Documentation: Detailed documentation covering design, architecture, deployment instructions, and usage guidelines.

### 2. [Azure Serverless Data Processing pipeline](https://github.com/skmahaboob/Azure-Serverless-Data-Processing-Pipeline)
**Description**: 
Azure Serverless Data Processing PipelineAzure Serverless Data Processing Pipeline

Associated with ModakAssociated with Modak
In data engineering, efficiency and scalability are key. My Azure Serverless Data Processing Pipeline project showcases these principles using serverless computing and cloud technologies. Designed to process JSON data from HTTP endpoints, this solution efficiently handles high data volumes with minimal infrastructure management.

**Objectives:**
- Real-Time Ingestion: Receive and process data from HTTP endpoints in real-time.
- Data Transformation: Clean, transform, and validate data before storage.
- Scalable Architecture: Use serverless technologies for automatic scaling.
- Reliable Storage: Store data securely in Azure SQL Database.

**Technologies:**
- Azure Functions: Serverless compute service for event-driven code execution.
- Azure SQL Database: Managed relational database with high availability.
- Python: For developing Azure Functions.
- pyodbc: Connects Python with Azure SQL Database.

**Implementation Phases:**
1. Design: Architected the solution with Azure Functions for data ingestion and Azure SQL Database for storage.
2. Development: Created an HTTP-triggered Azure Function to handle and process JSON data.
3. Testing: Verified functionality through unit, integration, local, and Azure tests.
4. Deployment: Deployed code to Azure, set up monitoring, and ensured smooth operation.

**Benefits:**
- Scalability: Automatic scaling with Azure Functions.
- Cost Efficiency: Pay-as-you-go model.
- Real-Time Processing: Supports up-to-date decision-making.
- Reduced Overhead: Minimal infrastructure management.



## 📜 Certifications
  - *Microsoft Certified Azure Fundamentals (AZ-900)* 🎓
  - *StreamSets White Belt Certification* 🥋

## 📄 Resume

You can download my resume [here](https://github.com/skmahaboob/skmahaboob.github.io/blob/c650275c735db0385fe5f65fb3938af945e163dc/Mahaboob_Sheik_Data_Engineer.pdf).

## 🌐 Website

Visit my live portfolio website at [https://skmahaboob.github.io](https://skmahaboob.github.io) to learn more about me, my skills, and my work.

---

Feel free to explore the repository and contact me if you have any questions or would like to collaborate on a project!

**Contact**: MahaboobSheik26@gmail.com 📧
```
